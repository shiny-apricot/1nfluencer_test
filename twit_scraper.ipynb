{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning and Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yasin\\AppData\\Local\\Temp\\ipykernel_18740\\1432473345.py:11: DeprecationWarning: Selenium Tools for Microsoft Edge is deprecated. Please upgrade to Selenium 4 which has built-in support for Microsoft Edge (Chromium): https://docs.microsoft.com/en-us/microsoft-edge/webdriver-chromium/#upgrading-from-selenium-3\n",
      "  driver = Edge(options=options)\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import csv\n",
    "from time import sleep\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from msedge.selenium_tools import Edge, EdgeOptions\n",
    "\n",
    "options = EdgeOptions()\n",
    "options.use_chromium = True\n",
    "# options.add_argument(\"--disable-infobars\")\n",
    "driver = Edge(options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://twitter.com/login\")\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = driver.find_element_by_xpath('//input[@name=\"text\"]')\n",
    "email.send_keys(\"yasininal44@gmail.com\")\n",
    "email.send_keys(Keys.RETURN)\n",
    "sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = driver.find_element_by_xpath('//input[@name=\"text\"]')\n",
    "username.send_keys(\"yasininall\")\n",
    "username.send_keys(Keys.RETURN)\n",
    "sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_password = getpass(\"Enter your password: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = driver.find_element_by_xpath('//input[@name=\"password\"]')\n",
    "password.send_keys(inp_password)\n",
    "password.send_keys(Keys.RETURN)\n",
    "sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with ronaldo profile\n",
    "driver.get(\"https://twitter.com/cristiano\")\n",
    "sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Username\n",
    "- Displayed Name\n",
    "- Description\n",
    "- Number of Followers\n",
    "- Number of Following\n",
    "- Birthday\n",
    "- Data that joined Twitter\n",
    "- Website\n",
    "- Within the last 50 posts\n",
    "  - Number of favorites\n",
    "  - Number of retweets\n",
    "  - Number of replies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "   \n",
    "def get_tweet_data(tweet):\n",
    "    try:\n",
    "        tweet_text = tweet.find_element_by_xpath('.//div[@data-testid=\"tweetText\"]').text\n",
    "        tweet_hash = hash(tweet_text.strip())\n",
    "    except:\n",
    "        tweet_hash = None\n",
    "\n",
    "    try:\n",
    "        favorites = tweet.find_element_by_xpath('.//div[@data-testid=\"like\"]').text\n",
    "        if favorites == \"\": favorites = 0\n",
    "    except: favorites = 0\n",
    "    \n",
    "    try:\n",
    "        retweet = tweet.find_element_by_xpath('.//div[@data-testid=\"retweet\"]').text\n",
    "        if retweet == \"\": retweet = 0\n",
    "    except: retweet = 0\n",
    "    \n",
    "    try:\n",
    "        reply = tweet.find_element_by_xpath('.//div[@data-testid=\"reply\"]').text\n",
    "        if reply == '': reply = 0\n",
    "    except: reply = 0\n",
    "    \n",
    "    # get tweet date\n",
    "    try:\n",
    "        tweet_date = tweet.find_element_by_xpath('.//time').get_attribute(\"datetime\")\n",
    "        # convert to datetime object\n",
    "        tweet_date = datetime.datetime.strptime(tweet_date, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    except: tweet_date = None\n",
    "        \n",
    "    return [tweet_hash, reply, retweet, favorites, tweet_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets(driver):\n",
    "    tweet_list = []\n",
    "    # get all tweets\n",
    "    tweets = driver.find_elements_by_xpath('//article[@data-testid=\"tweet\"] | //div[@data-testid=\"tweet\"]')\n",
    "    for tweet in tweets:\n",
    "        data = get_tweet_data(tweet)\n",
    "        # tweet_id = ''.join(data[0])\n",
    "        # data.append(tweet_id)\n",
    "        tweet_list.append(data)\n",
    "    return tweet_list\n",
    "        \n",
    "def get_tweet_count(driver):\n",
    "    tweets = driver.find_elements_by_xpath('//article[@data-testid=\"tweet\"] | //div[@data-testid=\"tweet\"]')\n",
    "    return len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_profile_info(driver):\n",
    "    # find a href that contains \"followers\"\n",
    "    follower = driver.find_element_by_xpath('//a[contains(@href, \"/followers\")]')\n",
    "    \n",
    "    # if the text contains 'K' letter, then it has more than 10000 followers\n",
    "    if follower.text.find('K') != -1 or follower.text.find('M') != -1:\n",
    "        # get the username from the link\n",
    "        username = follower.get_attribute('href').split('/')[3]\n",
    "        # print(\"USERNAME=\",username)\n",
    "        \n",
    "        displayed_name = driver.find_element_by_xpath('//div[@data-testid=\"UserName\"]').text\n",
    "        displayed_name = displayed_name.split('\\n')[1]\n",
    "        # get the second line of displayed name\n",
    "        # print(\"DISP_NAME=\",displayed_name)\n",
    "        \n",
    "        try:\n",
    "            # get description by skipping the line starting with '@'\n",
    "            description = driver.find_element_by_xpath('//div[@data-testid=\"UserDescription\"]').text\n",
    "        except: description = \"\"\n",
    "        \n",
    "        # get the number of followers\n",
    "        followers = follower.text.split(' ')[0].split('.')[0]\n",
    "        # print('FOLLOWERS=',followers)\n",
    "        \n",
    "        # get the number of following\n",
    "        following = driver.find_element_by_xpath('//a[contains(@href, \"/following\")]').text\n",
    "        following = following.split(' ')[0].split('.')[0]\n",
    "        # print('FOLLOWING=',following)\n",
    "        \n",
    "        try:\n",
    "            # get joined date\n",
    "            joined = driver.find_element_by_xpath('//span[@data-testid=\"UserJoinDate\"]').text\n",
    "            # get the text after the 'joined'\n",
    "            joined = joined.split('Joined')[1]\n",
    "        except: joined = \"\"\n",
    "        \n",
    "        try:\n",
    "            # get user url with 'UserUrl'\n",
    "            user_url = driver.find_element_by_xpath('//a[@data-testid=\"UserUrl\"]//span').text\n",
    "        except: user_url = \"\"\n",
    "        \n",
    "        return [username, displayed_name, description, followers, following, joined, user_url]\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_following_list(driver):\n",
    "    last_append = time.time()\n",
    "    following_list = []\n",
    "    while True:\n",
    "        random_users = driver.find_elements_by_xpath('//div[@data-testid=\"UserCell\"]')\n",
    "        for usr in random_users:\n",
    "            username = usr.text.split('\\n')[1].split('@')[1]\n",
    "            if username not in following_list:\n",
    "                following_list.append(username)\n",
    "                last_append = time.time()\n",
    "        driver.execute_script(\"window.scrollBy(0,1500);\")\n",
    "        sleep(1)\n",
    "        if len(following_list) > 55 or time.time() - last_append > 5:\n",
    "            print(f\"{len(following_list)} random usernames are fetched\")\n",
    "            return following_list\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 random usernames are fetched\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tweet_count = 0\n",
    "accepted_tweets = []\n",
    "id_list = []\n",
    "random_user_list = []\n",
    "last_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "# scrolling = True\n",
    "\n",
    "spent_time_on_user = time.time()\n",
    "\n",
    "while True:\n",
    "    profile_info = get_profile_info(driver)\n",
    "    if profile_info is None or time.time() - spent_time_on_user > 150:\n",
    "        driver.get(f\"https://twitter.com/{random_user_list[-1]}\")\n",
    "        spent_time_on_user = time.time()\n",
    "        # delete last item of the list\n",
    "        random_user_list.pop()\n",
    "    \n",
    "    if tweet_count >= 50: \n",
    "        # save tweets to csv along \n",
    "        with open(f'tweets/{profile_info[0]}.csv', 'a') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(accepted_tweets) \n",
    "        tweet_count = 0\n",
    "        sleep(4)\n",
    "        if len(random_user_list) == 0:\n",
    "            # go following list page that includes \"/following\" text\n",
    "            driver.get(f\"https://twitter.com/{profile_info[0]}/following\")\n",
    "            random_user_list = get_following_list(driver)\n",
    "        else:\n",
    "            driver.get(f\"https://twitter.com/{random_user_list[-1]}\")\n",
    "            sleep(5)\n",
    "            spent_time_on_user = time.time()\n",
    "            # delete last item of the list\n",
    "            random_user_list.pop()\n",
    "            \n",
    "    tweets = get_all_tweets(driver)\n",
    "    \n",
    "    for tw in tweets:\n",
    "        if tw[0] not in id_list:\n",
    "            id_list.append(tw[0])\n",
    "            tweet_count += 1\n",
    "            accepted_tweets.append(tw)  \n",
    "            \n",
    "    scroll_attempt = 0\n",
    "    # scroll down \"a little bit\" to load more tweets\n",
    "    driver.execute_script(\"window.scrollBy(0,1500);\")\n",
    "    sleep(1)\n",
    "    current_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "    if current_position == last_position:\n",
    "        scroll_attempt += 1\n",
    "        if scroll_attempt >= 4:\n",
    "            # scrolling = False\n",
    "            break\n",
    "        else:\n",
    "            sleep(2)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def profile_info_to_csv(profile_info):\n",
    "    profile_list = []\n",
    "    # iterate through the csv files in profiles folder\n",
    "    for file in glob.glob(os.path.join('tweets', '*.csv')):\n",
    "        # read filename before the .csv\n",
    "        username = file.split('\\\\')[1].split('.')[0]\n",
    "        driver.get(f\"https://twitter.com/{username}\")\n",
    "        sleep(4)\n",
    "        profile_info = get_profile_info(driver)\n",
    "        if profile_info is None:\n",
    "            print(\"profile info is None\")\n",
    "            continue\n",
    "        profile_list.append(profile_info)\n",
    "        \n",
    "    with open(f'profile_info.csv', 'a', encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(profile_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'profile_info.csv', 'a', encoding=\"utf-8\") as f:\n",
    "    header = ['username', 'displayed_name', 'description', 'followers', 'following', 'joined', 'user_url']\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(profile_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get all the csv files in the tweets folder and concatenate them into one dataframe\n",
    "# skip empty lines\n",
    "header = ['tweet_hash', 'reply', 'retweet', 'favorites', 'tweet_date']\n",
    "df = pd.DataFrame()\n",
    "for file in glob.glob(os.path.join('tweets', '*.csv')):\n",
    "    file = pd.read_csv(file, skip_blank_lines=True, encoding=\"utf-8\", names=header)\n",
    "    df = pd.concat([df, file], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_hash</th>\n",
       "      <th>reply</th>\n",
       "      <th>retweet</th>\n",
       "      <th>favorites</th>\n",
       "      <th>tweet_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2,092</td>\n",
       "      <td>6,748</td>\n",
       "      <td>129K</td>\n",
       "      <td>2022-05-31 18:17:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.792970e+18</td>\n",
       "      <td>1,951</td>\n",
       "      <td>2,727</td>\n",
       "      <td>33.4K</td>\n",
       "      <td>2022-05-28 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-7.071670e+18</td>\n",
       "      <td>8,890</td>\n",
       "      <td>29.1K</td>\n",
       "      <td>623.7K</td>\n",
       "      <td>2022-05-16 19:27:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_hash  reply retweet favorites           tweet_date\n",
       "0  0.000000e+00  2,092   6,748      129K  2022-05-31 18:17:23\n",
       "1  8.792970e+18  1,951   2,727     33.4K  2022-05-28 18:00:00\n",
       "2 -7.071670e+18  8,890   29.1K    623.7K  2022-05-16 19:27:16"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36493"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_date\n",
       "2014-12-22       7\n",
       "2014-12-24       7\n",
       "2015-01-13       7\n",
       "2015-01-17       7\n",
       "2015-01-20       7\n",
       "              ... \n",
       "2022-05-27     543\n",
       "2022-05-28     679\n",
       "2022-05-29     607\n",
       "2022-05-30    1070\n",
       "2022-05-31    1313\n",
       "Name: tweet_hash, Length: 610, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn tweet_date into datetime\n",
    "df['tweet_date'] = pd.to_datetime(df['tweet_date'])\n",
    "# calculate number of tweets per day\n",
    "# and save it as a different dataframe\n",
    "# calculate tweets, replies, retweets, favorites seperately\n",
    "df['tweet_date'] = pd.to_datetime(df['tweet_date'])\n",
    "df_per_day = df.groupby(df['tweet_date'].dt.date).count()\n",
    "df_per_day.tweet_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yasin\\AppData\\Local\\Temp\\ipykernel_18740\\2271000905.py:2: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead.\n",
      "  df_per_week = df.groupby(df['tweet_date'].dt.week).count()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweet_date\n",
       "1      946\n",
       "2      833\n",
       "3     1150\n",
       "4      682\n",
       "5      548\n",
       "6      532\n",
       "7      450\n",
       "8      362\n",
       "9      367\n",
       "10     490\n",
       "11     507\n",
       "12     373\n",
       "13     546\n",
       "14     675\n",
       "15    1174\n",
       "16    1745\n",
       "17    1208\n",
       "18    2681\n",
       "19    2230\n",
       "20    2895\n",
       "21    5346\n",
       "22    2681\n",
       "23     239\n",
       "24     167\n",
       "25      76\n",
       "26     297\n",
       "27     163\n",
       "28      93\n",
       "29      96\n",
       "30     116\n",
       "31      69\n",
       "32     146\n",
       "33      41\n",
       "34      69\n",
       "35     108\n",
       "36      56\n",
       "37     104\n",
       "38     109\n",
       "39     126\n",
       "40      68\n",
       "41     206\n",
       "42     121\n",
       "43     165\n",
       "44     373\n",
       "45     447\n",
       "46     284\n",
       "47     284\n",
       "48     562\n",
       "49     890\n",
       "50     859\n",
       "51     812\n",
       "52     802\n",
       "53      76\n",
       "Name: tweet_hash, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate number of tweets per week\n",
    "df_per_week = df.groupby(df['tweet_date'].dt.week).count()\n",
    "df_per_week.tweet_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_date\n",
       "1      3801\n",
       "2      1917\n",
       "3      2109\n",
       "4      4802\n",
       "5     15799\n",
       "6       798\n",
       "7       619\n",
       "8       330\n",
       "9       459\n",
       "10      614\n",
       "11     1604\n",
       "12     3593\n",
       "Name: tweet_hash, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate number of tweets per year and month\n",
    "df_per_month = df.groupby(df['tweet_date'].dt.month).count()\n",
    "df_per_month.tweet_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59.74590163934426, 687.6415094339623, 3037.0833333333335)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mean of tweets per day, week and month\n",
    "df_per_day.tweet_hash.mean(), df_per_week.tweet_hash.mean(), df_per_month.tweet_hash.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.0, 373.0, 1760.5)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate median of tweets per day, week and month\n",
    "df_per_day.tweet_hash.median(), df_per_week.tweet_hash.median(), df_per_month.tweet_hash.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***I think median value would represent a profile better since it is much more resistant to outlier values.***"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc615f8fa9a83d26fab2405a304dbee89495174b6c63caed719e901ce2732455"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
